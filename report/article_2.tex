%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Journal Article
% LaTeX Template
% Version 1.4 (15/5/16)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com) with extensive modifications by
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[twoside,twocolumn]{article}

\usepackage{blindtext} % Package to generate dummy text throughout this template 

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage[english]{babel} % Language hyphenation and typographical rules

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt,a4paper,total={6in,9in}]{geometry} % Document margins
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text

\usepackage{enumitem} % Customized lists
\setlist[itemize]{noitemsep} % Make itemize lists more compact

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\roman{subsection}} % roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{Game of Life  $\bullet$ Computer Systems A $\bullet$ December 2022} % Custom header text
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{titling} % Customizing the title section

\usepackage{hyperref} % For hyperlinks in the PDF
\usepackage{graphicx}
\graphicspath{{./img/}}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{txfonts}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip} % Move the title up

\pretitle{\begin{center}\Huge\bfseries} % Article title formatting
\posttitle{\end{center}} % Article title closing formatting
\title{A parallelised and distributed approach to implementing Conway's Game of Life} % Article title
\author{%
\textsc{Aaron Chan}\\[1ex] % Your name
\normalsize University of Bristol \\ % Your institution
\normalsize {ho21739@bristol.ac.uk} % Your email address
\and % Uncomment if 2 authors are required, duplicate these 4 lines if more
\textsc{Ferdinand Hubbard}\\[1ex] % Second author's name
\normalsize University of Bristol \\ % Your institution
\normalsize {ej21378@bristol.ac.uk} % Your email address
}
\date{\today} % Leave empty to omit a date
\renewcommand{\maketitlehookd}{
\begin{abstract}
  We intend to explain our two solutions for implementing Conway's Game of Life in Golang, namely the Parallel and Distributed versions.
  Within this report, we will discuss the impacts of thread usage in the parallel implementation, as well as the effect of distributed computation as opposed to single-machine computation. In essence, we found that execution time was sped up as we increased thread count, reduced branch instructions and implemented a halo exchange mechanism.
\end{abstract}
}

%----------------------------------------------------------------------------------------

\begin{document}

% Print the title
\maketitle

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------
\section{Introduction to the parallel solution}

As an initial attempt, we began by implementing a 
single-threaded Game of Life (GoL) Engine, expanding upon this work 
with parallelisation by virtue of concurrent go-routine workers. 
Recognising that there exists various methods to calculating the perimeter
for \texttt{getNeighbourCount}, we tried various counting methods - taking into 
account the modularity of Game of Life - as well as finding channel alternatives.

% \begin{itemize}
%   \item single threaded implementation
%   \item events passing (alive cells, turn completed)
%   \item PGM image output implementation
%   \item SDL implementation (flipped cells events)
%   \item using channels and goroutines to create multiple workers (parallelising the solution)
%   \item switching to a memory sharing solution instead of using channels (TODO?)
%   \item implementing halo exchange
% \end{itemize}
%------------------------------------------------

\section{benchmarking methodology}
Our means of evaluating parallelisation performance gains 
was via the Go testing framework. Using 512px x 512px PGM images, we 
repeated each benchmark 10 times, taking the average runtime for each thread to acquire the central value, 
and plotting them. Each feature implementation was benchmarked in the same fashion, using Linux lab machines 
with CPUs being Intel i7-8700 (6 core, 12 threads).
% \lettrine[nindent=0em,lines=3]{L} orem ipsum dolor sit amet, consectetur adipiscing elit.
% \begin{itemize}
%   \item use built in golang benchmark framework
%   \item run command ....
%   \item output to txt
%   \item run python script to plot graphs
%   \item barchart of time taken against threads used
%   \item runs benchmarks on all pgm images
%   \item repeats benchmarks until results fall within a certain standard deviation (TBC)
% \end{itemize}
%------------------------------------------------
\renewcommand\thesubsection{\Alph{subsection}}
\begin{figure}
  \includegraphics[width=\linewidth]{modulo.png}
  \caption{Modulo Implementation}
  \label{fig:chart1}
\end{figure}

\section{A split personality program}
\subsection{Parallelisation logic}
The baseline implementation's distributor function starts with
reading the image into 2D slice via \textit{io} commands, before evenly dividing 
the workload between workers. With the aid of go-routines, workers have access to the old slice
as well as the boundaries in which to process. By using \texttt{getNeighbourCount}, we apply the Game of Life
laws onto the world, piping results into a blank slice of size: \[( endRow - startRow ) \times ImageWidth\]
Sending the results back to the distributor, we integrate back together the new slices, move the pointer to the old world
to this newly-merged world and send the appropriate signals on the \textit{events}, \textit{io} channels.

%------------------------------------------------

\subsection{On the topic of Analysis 1}

From Fig. 1, we can conclude that multithreading have an overall improvement on performance. 
There is a correlation between increasing number of threads and decreasing execution times, likely due 
to some functions, such as \texttt{getNeighbourCount}, being "embarrassingly parallelisable". We 
favour the term "delightfully parallel". As we have more cores for processing, the time taken for each turn is 
reduced, hence the curve. 

We do take notice of the obvious trend of diminishing returns - 
the gradient of the time-thread curve flattens. We attribute this to unparallelisable core tasks such as IO 
- the major slowdown that is reading and writing to/from the disk, and submit that using more threads than 
a CPU physically has causes overhead in scheduling, minimising the benefits of allocating extra threads for processing.
We suspect also that, contributing to this, the workload division does not change as much the more threads there are,
which explains the reciprocal graph shape curve.

Regarding the numerical results, we find greatest performance was with 16 threads (despite the diminishing returns)
with a 67.6\% decrease in processing time. As for the worst change in performance thread-to-thread, 
we see the 14th - 15th threads having the smallest difference in performance change (0.353\%), but acknowledge the
potential anomalous data point.

\section{How to count?}
\subsection{Counting methods}
It is well documented that division and other non-atomic arithmetic operations are slow. Thus, we submit our alternative
implementations for \texttt{getNeighbourCount}. Fig. 1, our original implementation, uses the modulo operation (\%), 
whereas Fig. 2, Fig. 3, use Branching (if-statements) and Bitmasking respectively. From this data, we see
that Bitmasking is the fastest on average, but we can identify positive notes from all three.
\subsection{On the topic of Analysis 2}
\begin{figure}
  \includegraphics[width=\linewidth]{branching.png}
  \caption{Branching Implementation}
  \label{fig:chart2}
\end{figure}
\begin{figure}
  \includegraphics[width=\linewidth]{bitmask.png}
  \caption{Bitmask Implementation}
  \label{fig:chart3}
\end{figure}

Statistically-speaking, we find the solution using 
modulo operations is most affected by thread usage, having standard deviation 413ms, as opposed to 107ms 
and 73ms for branching and bitmasking respectively. Interested persons will find our results achieved by:
\[SD(\forall x \in runtimes : max(runtimes) - x)\]
As discussed before, this, however, implies not that higher threads means greater 
performance as there exists other, non-parallelisable tasks setting a base level of execution time. 
The low standard deviation of the bitmasked implementation alludes to this portion of the program being as 
near optimised as we possibly can. 

When it comes to average times for threads, bitmasking takes the lead with
890ms, followed by 1026ms and 1989ms for branching and modular respectively. This does identify branching for the 
greatest decrease in runtime, with a 74.2\% decrease, as opposed to 32.1\%.

\subsection{Our Algorithms}

Code-wise, our initial implementation for \texttt{getNeighbourCount} used modulo operators unnecessarily. It did the following:
\begin{enumerate}[noitemsep]
  \item Set row = coordinate row \% ImageHeight
  \item Set col = coordinate col \% ImageWidth
  \item If row | col < 0, then add ImageHeight/Width respectively
  \item If world[row][col] was 255, increment alive count
\end{enumerate}

\begin{algorithm}
  \caption{modulo getNeighbourCount function}
  \begin{algorithmic}
    \State Let $row \gets rowCoordinate \% ImageHeight$
    \State Let $col \gets colCoordinate \% ImageWidth$
    \If{row < 0} 
    \State $row \gets ImageHeight - 1$
    \ElsIf{col < 0}
    \State $row \gets ImageWidth - 1$ 
    \EndIf
    \If{alive[row][col] = 0xFF}
    \State $alive++$
    \EndIf
    \State \Return $alive$
  \end{algorithmic}
\end{algorithm}

With this, while it succeeds in its purpose, it does not
succeed efficiently. Areas of improvement include removal of the modulo operator, which, here,
does not serve any real purpose. This leads to our second implementation - branching:

\begin{algorithm}
  \caption{Branching getNeighbourCount}
  \begin{algorithmic}
    \State Let $i, j \gets (row, column)$ on world
  \State Let $O \gets [\{-1,0,1\} \times \{-1, 0, 1\} - \{0,0\}]$
  \State Let $alive \gets 0$   
  \For{$(x,y) \in O$}
      \State $row \gets (i + y)$
      \If{row < 0}
        \State $row \gets ImageHeight-1$
      \ElsIf{row = ImageHeight}
        \State $row \gets 0$
      \EndIf
      \State $col \gets (j + x)$
      \If{col < 0}
        \State $col \gets ImageWidth - 1$
        \ElsIf{col = ImageWidth}
          \State $col \gets 0$
      \EndIf
      \If{world[row][col] = 255}
      \State $alive++$
      \EndIf
    \EndFor
    \State \Return $alive$
  \end{algorithmic}
\end{algorithm}
Although the line count has certainly increased, the performance has drastically improved. Citing the 
74.2\% decrease, this does appear to be "good enough" as an implementation. An improvement may be to rid 
ourselves of branches, as this involves branch prediction which itself is intensive, due to how the CPU 
does not know which path will be taken ahead of time. If we use only logical operations,
we get the Bitmasking implementation:
\begin{algorithm}
  \caption{Bitmasked getNeighbourCount}
  \begin{algorithmic}
    \State Let $i, j \gets (row, column)$ on world
  \State Let $O \gets [\{-1,0,1\} \times \{-1, 0, 1\} - \{0,0\}]$
  \State Let $alive \gets 0$   
  \For{$(x,y) \in O$}
      \State $row \gets (i + y) \& (ImageHeight - 1)$
      \State $col \gets (j + x) \& (ImageWidth - 1)$
      \State $alive \gets alive + world[row][col] >> 7$
    \EndFor
    \State \Return alive
  \end{algorithmic}
\end{algorithm}

This algorithm is our fastest implementation. Minimising both line count and runtime, it 
counts the neighbours using purely bitwise operators and assignments - no branching needed.

%------------------------------------------------

\section{memory sharing}
\subsection{Motivation}
Looking at our achievements in counting, we noticed overhead associated with channel communication.
Exploring \href{https://github.com/golang/go/blob/4fc9565ffce91c4299903f7c17a275f0786734a1/src/runtime/chan.go}
{Go's implementation for channels}, we see much overhead. Since we know a channel is a thread-safe memory sharing
mechanism, we can rid the overhead, using only mutex locks and condition variables.
\subsection{Implementation}
Our homemade channel implementation does the following:
\begin{enumerate}[noitemsep]
  \item Acquire the mutex lock
  \item While \texttt{condition} is false, await broadcast unless non-blocking send
  \item Use the value, then broadcast to other processes
  \item Finally, unlock mutex
\end{enumerate}
\begin{figure}
  \includegraphics[width=\linewidth]{memshare.png}
  \caption{Memory Sharing}
  \label{fig:chart4}
\end{figure}
We introduce 2 methods, \texttt{Send} \& \texttt{Receive}. \texttt{Send} waits for the memory space 
to be empty, and replaces it with a value. \texttt{Receive} waits for the memory space to point to a value, and then 
replaces it with \texttt{nil}. \texttt{Go 1.12} does not have generics, which vastly complicated implementation. 
Benchmarks with memory sharing (and bitmasked \texttt{getNeighbourCount}) is shown in Fig. 4.
\subsection{On the topic of Analysis 3}
This data was not very analysable. The lack of clear trend and sporadic data points suggests no real difference 
in performance when adding memory sharing, or faulty implementation (which we realistically do consider a possibility).
A trend, if any, we identify is that the Modulo performance suffered, but due to the lack of clarity, this is inconclusive.
We add that repeated trials yielded similar style results. Our method used the following: \[\forall t \in T: ORIG[t] - MEM[t]\]
Where $MEM$ is memory sharing implementation time, $ORIG$ is channel impl. time, $T$ is the number of threads. 

\section{Distributed Implementation}
Similar to how we did the parallel side, we started the distributed section by using a single worker, this time
with the worker being a separate process and communicated to via Remote Procedure Calls (RPC henceforth). This expanded to
doing using multiple workers, implementing fault tolerance and finally halo exchange.

%------------------------------------------------
\section{A lonesome worker}
\subsection{Logic}
Our single-worker distributed implementation moved much of the processing to said worker. From the distributor, we sent a 
single RPC call to the worker, processed GoL turns on the worker, then sent the final state back. Results are shown in Fig. 5.
\begin{figure}
  \includegraphics[width=\linewidth]{1v1.png}
  \caption{1 worker vs. 1 thread}
  \label{fig:chart5}
\end{figure}
\subsection{On the topic of Analysis 4}
We decided to test single threaded parallel version against a single worker distributed GoL. Seeing that the 1 worker ran with
runtime 1362ms versus the single-threaded 1208ms, we concluded that the 154ms difference was due to the latency between Bristol
and AWS London, in addition to some potentially contributing overhead from RPC calls. The rest of the runtime we concluded was the natural
runtime of the worker processing Game of Life. 

\section{Friends? - Multi-worker GoL}
\subsection{Implementing multiple workers}
With the introduction of multiple workers, we needed to introduce a broker to interface the workers for the controller
and vice versa. This did require moving turn iteration logic to the broker, requiring a major refactor. In the end, the system
revolved around the broker with bidirectional RPC communication betweeen workers and the controller. We equally divide work
between workers, and all processes keep an internal state of some sort. By massively simplifying, the broker does the following:

\begin{enumerate}[noitemsep]
  \item Listen for worker and controller connections
  \item On connect, call the worker/controller back (bidirectional)
  \item Listen for GoL process request from controller
  \item Iterate, dividing and sending work to each worker.
  \item After getting all parts back together, apply changes to broker internal state and divide new state up
\end{enumerate}
In order to call the worker back, this meant that the worker needed to provide its own IP address - the same 
process happens for the controller. The reason why the worker needs bidirectional communication is that later on with halo-exchange,
we will need this to push the state when the turn iterator is at the worker, and also to allow workers to disconnect 
without errors.

When it comes to the reassembly, the worker sends the flipped cells to the broker, and because no 
worker interferes with another worker's section, we can just apply the changes in any order to the broker's
internal state.
%--------------
\begin{figure}
  \includegraphics[width=\linewidth]{multi-worker.png}
  \caption{Multi-worker local performance}
  \label{fig:chart6}
\end{figure}
%--------------
\subsection{Analysis 5}
%--------------
Shown in Fig. 6 is the distributed system when run on a local machine (due to port-forwarding issues).
We can see a general negative correlation between workers and runtime - as workers increases, the runtime decreases.
Noticing that the runtime begins to incline at the end, we initially thought this was due to RPC overhead, but it may have been
due to how running on 1 machine starts to cost more performance than it improves.

\section{Tolerating faults}
In our previous implementation, we never checked for worker disconnects. This leaves us in a vulnerable
position - if a worker were to be terminated at any point, then we would have one section of changes missing.
This would show itself with a static portion on the SDL window. There is also the case of adding more workers. Either
way, it means we need to keep the order of workers. We present Algorithm 3 to solve this.
\begin{algorithm}
  \caption{Worker-diff checker}
  \begin{algorithmic}
    \State Require $ids \subseteq \mathbb{N}$
    \State Require $oldIds \subseteq \mathbb{N}$ 
    \State Let $idMap \gets Map$
    \For{$id \in ids$}
      \State $idMap[id] \gets addr$
    \EndFor
  \For{$id \in oldIds$}
      \If{idMap[id] = nil}
      \State \Return False
      \EndIf
    \EndFor
    \State \Return True
  \end{algorithmic}
\end{algorithm}


What this algorithm achieves is detection of disconnects and reconnects.
Given that we have the loop invariant (in turn iterator) "no two workers will have the same worker id",
we can use this algorithm. If we detect a change, recalibrate the workers to adjust for new/missing worker. 
One may ask, does a worker disconnect not throw errors when calling RPC methods? In this case, we utilise \texttt{client.Go},
which notifies the calling process of errors without crashing. So, if error, then trigger recalibration by resetting each worker's
internal state.

More specifically, the broker sends an RPC call to each remaining worker with a fresh state from which to 
evolve from. The previous internal state of each worker is discarded, even if it is one turn ahead as we use the broker's
internal state, which is only pushed to if all workers return their flipped cells fine.
\begin{figure}
  \includegraphics[width=\linewidth]{halo.png}
  \caption{Multi-worker AWS performance}
  \label{fig:chart7}
\end{figure}

\section{Implementing Halo Exchange}
Now that we have a broker and a Publish-Subscriber model, we can further optimise by having halo exchange.
\subsection{How Jesus Christ came to be}
Our first attempt wasn't exactly the definition of halo exchange. It used halos distributed by broker to
each worker, who then used the halos to update their internal state. Now, our implementation transfers the turn iterator
logic to the workers. The sequence of events happens as follows:
\begin{enumerate}[noitemsep]
  \item Worker connects to broker, and is initialised with parameters of controller.
  \item On initialisation, worker will dial the top and bottom neighbours (if any), then await work.
  \item On work, send its halos, and evolve its own slice given the neighbouring halos
  \item Results are pushed to broker and loop
\end{enumerate}
This oversimplified view of halo exchange is represented by a significant amount of implementation code.
We present our results with Fig. 6.


\subsection{Analysis 6 - Judgement Day}
The absurd difference in value between Fig. 6, Fig. 7, was due to what we identified as transmission latency. The broker
was positioned in \texttt{us-east-1}, North Virginia, and the 7000km+ differential caused the latency 
to be very high.
We did notice, despite the glaringly obvious latency issue, that the time did start to decline as more workers
were added, implying that there may have been some speed increases due to our distributed system. However,
we do not reject the notion of these being oscillations in latency values.

Giving the benefit of the doubt, had the EC2 instances been set up in the correct region, then we should have noticed
a significantly smaller runtime with a lower standard deviation - that is, less variance in data points.
%------------------------------------------------


%------------------------------------------------

\section{Areas for improvement}

\begin{itemize}
  \item Memory sharing implementation for channels - we were able to create an alternative to channels, but
  this ended up more or less the same performance as a regular channel, if not worse. A strong area of improvement
  would be to rewrite this, and substitute all the other channels used with our implementation
  \item Parallelising workers - we did not manage to implement this in time. Since all the code for multithreading
  was in our parallel code, we could have imported the code then implemented it inside our workers.
  \item Controller on AWS node - we found issues with latency when using AWS nodes, so if the "local" controller was
  localised in the AWS data center, then we would eliminate this issue. We, however, ran into issues with SDL not working
  on EC2 machines and port-forward which meant these latency issues still happened.
  \item Broker fault tolerance - As a extreme extension, we have the broker as our single point of failure. If the broker fails, then
  nothing can connect and no processes can occur. We could, in theory, create a second broker to be redundant in case the instance 
  running the initial broker fails.
\end{itemize}
\subsection{Thanks}
\begin{itemize}
  \item Lecturers and TAs for their support
  \item Monster Energy\texttrademark
  \item My missing sanity
\end{itemize}

%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------


%----------------------------------------------------------------------------------------

\end{document}